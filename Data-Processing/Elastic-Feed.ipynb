{
 "cells": [
  {
   "cell_type": "raw",
   "id": "12a409b6-0d8b-4624-8f59-4417cc3e3f64",
   "metadata": {},
   "source": [
    "Welcome to SageAI\n",
    "The Dataset-final.csv is the final CSV contaning the Hashbinaries of the products, with proper formatting of the name and description\n",
    "\n",
    "Now to install Elastic and Kibana on your device run the following commands.\n",
    "\n",
    "export ELASTIC_PASSWORD=\"123456\"\n",
    "\n",
    "curl -fsSL https://elastic.co/start-local | sh\n",
    "\n",
    "After running the script, you can access Elastic services at the following endpoints:\n",
    "Elasticsearch: http://localhost:9200\n",
    "Kibana: http://localhost:5601"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bfd766-d58e-4d30-ad21-36b012846b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# Connect to Elasticsearch\n",
    "es = Elasticsearch(\n",
    "    \"http://localhost:9200\",\n",
    "    basic_auth=(\"elastic\", \"123456\")  # Replace with your actual credentials\n",
    ")\n",
    "\n",
    "# Load your CSV file\n",
    "final_df = pd.read_csv('Dataset-final.csv')\n",
    "\n",
    "# Function to convert binary hash to a list of integers\n",
    "def binary_hash_to_list(binary_hash):\n",
    "    return [int(bit) for bit in binary_hash]\n",
    "\n",
    "# Apply the conversion to the binary_hash column\n",
    "final_df['hashed_vector'] = final_df['binary_hash'].apply(binary_hash_to_list)\n",
    "\n",
    "# Define the synonym list\n",
    "synonyms = [\n",
    "    \"laptop, notebook\",\n",
    "    \"tv, television\",\n",
    "    \"cellphone, smartphone, mobile phone\",\n",
    "    \"headphones, headset\",\n",
    "    \"camera, camcorder\",\n",
    "    # Add more synonyms as needed\n",
    "]\n",
    "\n",
    "# Define the index mapping and settings\n",
    "index_name = 'products2'\n",
    "\n",
    "# First, delete the index if it exists\n",
    "if es.indices.exists(index=index_name):\n",
    "    es.indices.delete(index=index_name)\n",
    "\n",
    "# Create the index with custom analyzers and mappings\n",
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            \"filter\": {\n",
    "                \"english_stop\": {\n",
    "                    \"type\": \"stop\",\n",
    "                    \"stopwords\": \"_english_\"\n",
    "                },\n",
    "                \"english_stemmer\": {\n",
    "                    \"type\": \"stemmer\",\n",
    "                    \"language\": \"english\"\n",
    "                },\n",
    "                \"english_possessive_stemmer\": {\n",
    "                    \"type\": \"stemmer\",\n",
    "                    \"language\": \"possessive_english\"\n",
    "                },\n",
    "                \"synonym_filter\": {\n",
    "                    \"type\": \"synonym\",\n",
    "                    \"synonyms\": synonyms\n",
    "                }\n",
    "            },\n",
    "            \"analyzer\": {\n",
    "                \"custom_english_analyzer\": {\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\",\n",
    "                        \"english_possessive_stemmer\",\n",
    "                        \"english_stop\",\n",
    "                        \"english_stemmer\",\n",
    "                        \"synonym_filter\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"product_id\": {\"type\": \"keyword\"},\n",
    "           \"hashed_vector\": {\n",
    "              \"type\": \"dense_vector\",\n",
    "              \"dims\": 64  # Replace 128 with the number of dimensions in your vectors\n",
    "            },\n",
    "            \"product_url\": {\"type\": \"keyword\"},\n",
    "            \"product_description\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"custom_english_analyzer\"\n",
    "            },\n",
    "            \"product_name\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"custom_english_analyzer\"\n",
    "            },\n",
    "            \"images\": {\"type\": \"keyword\"},\n",
    "            \"price\": {\"type\": \"float\"}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create the index\n",
    "es.indices.create(index=index_name, body=index_settings)\n",
    "\n",
    "# Function to index documents\n",
    "def index_documents(df, index_name):\n",
    "    print(f\"Starting the indexing process for index: {index_name}\")\n",
    "    success_count = 0\n",
    "    error_count = 0\n",
    "    for _, row in df.iterrows():\n",
    "        # Check if 'images' is a string and split if possible, else set to an empty list\n",
    "        images = row['images'].split(' | ') if isinstance(row['images'], str) else []\n",
    "\n",
    "        document = {\n",
    "            \"product_id\": row['product_id'],\n",
    "            \"hashed_vector\": row['hashed_vector'],  # This should be a list of integers\n",
    "            \"product_url\": row['Producturl'],\n",
    "            \"product_description\": row['description'],\n",
    "            \"product_name\": row['name'],\n",
    "            \"images\": images,  # Use the split images or empty list\n",
    "            \"price\": row['price']\n",
    "        }\n",
    "        try:\n",
    "            # Index the document\n",
    "            es.index(index=index_name, body=document)\n",
    "            # print(f\"Successfully indexed document: {row['product_id']}\")\n",
    "            success_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error indexing document {row['product_id']}: {e}\")\n",
    "            error_count += 1\n",
    "\n",
    "    print(f\"Indexing completed. Successfully indexed {success_count} documents. {error_count} errors occurred.\")\n",
    "\n",
    "# Index the documents\n",
    "index_documents(final_df, index_name)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8bc0f669-ade3-4e36-b185-72d0e985c46f",
   "metadata": {},
   "source": [
    "After Running the above block your elastic local instance will have a index named products2, which will have your data.\n",
    "\n",
    "Now the next block is for fethcing the results from the elastic search\n",
    "For running the above and the next block you need the following files in the same directory:\n",
    "1.Dataset-final.csv\n",
    "2.pca_model.joblib\n",
    "3.rotation_matrix.npy\n",
    "4.embeddings_mean.npy\n",
    "\n",
    "Make sure you have all these files present before you run the next block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed785dc4-fe6d-470a-b954-10c4e52ea113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from elasticsearch import Elasticsearch\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import faiss\n",
    "\n",
    "# Connect to Elasticsearch\n",
    "es = Elasticsearch(\n",
    "    \"http://localhost:9200\",\n",
    "    basic_auth=(\"elastic\", \"123456\")  # Replace with your actual credentials\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('Dataset-final.csv')\n",
    "# 2. Prepare the hash codes\n",
    "def process_binary_hashes(binary_hash_str):\n",
    "    return np.array(list(map(int, binary_hash_str.strip())), dtype=np.uint8)\n",
    "\n",
    "df['binary_code'] = df['binary_hash'].apply(process_binary_hashes)\n",
    "\n",
    "binary_codes = np.vstack(df['binary_code'].values)\n",
    "\n",
    "def pack_binary_codes(binary_codes):\n",
    "    binary_codes_packed = np.packbits(binary_codes, axis=1)\n",
    "\n",
    "    return binary_codes_packed\n",
    "\n",
    "binary_codes_packed = pack_binary_codes(binary_codes)\n",
    "\n",
    "# 3. Build the binary index\n",
    "num_bits = binary_codes.shape[1]\n",
    "index = faiss.IndexBinaryFlat(num_bits)\n",
    "index.add(binary_codes_packed)\n",
    "\n",
    "index_name = 'products2'  # Your existing index name\n",
    "pca = joblib.load('pca_model.joblib')\n",
    "rotation_matrix = np.load('rotation_matrix.npy')\n",
    "embeddings_mean = np.load('embeddings_mean.npy')\n",
    "# Load the embedding model (replace with your actual model)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "def query_to_hashed_vector(query):\n",
    "    \"\"\"\n",
    "    Convert query text to a hashed vector using the same process as the dataset.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query text.\n",
    "\n",
    "    Returns:\n",
    "        binary_code_list (list): The binary hash code of the query as a list of integers.\n",
    "    \"\"\"\n",
    "    # Step 1: Generate the embedding\n",
    "    query_embedding = embedding_model.encode(query)\n",
    "\n",
    "    # Step 2: Apply PCA transformation\n",
    "    query_embedding_pca = pca.transform([query_embedding])[0]\n",
    "\n",
    "    # Step 3: Zero-center the embedding using embeddings_mean from the dataset\n",
    "    centered_query_embedding = query_embedding_pca - embeddings_mean\n",
    "\n",
    "    # Step 4: Apply the ITQ rotation\n",
    "    rotated_query_embedding = np.dot(centered_query_embedding, rotation_matrix)\n",
    "\n",
    "    # Step 5: Binarize (quantize)\n",
    "    binary_code = (rotated_query_embedding > 0).astype(int)\n",
    "\n",
    "    # Convert binary code to list of integers\n",
    "    binary_code_list = binary_code.tolist()\n",
    "\n",
    "    return binary_code_list\n",
    "\n",
    "def keyword_search(query_text, index_name):\n",
    "    # Perform the keyword search\n",
    "    keyword_query = {\n",
    "        \"query\": {\n",
    "            \"multi_match\": {\n",
    "                \"query\": query_text,\n",
    "                \"fields\": [\n",
    "                    \"product_name\",  # Boost product_name field\n",
    "                    \"product_description^3\"\n",
    "                ],\n",
    "                # \"analyzer\": \"your_custom_analyzer\",  # Uncomment if using a custom analyzer\n",
    "                \"type\": \"best_fields\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    keyword_response = es.search(index=index_name, body=keyword_query)\n",
    "    return keyword_response\n",
    "\n",
    "\n",
    "\n",
    "def semantic_search_hash(query, index, df, k=10):\n",
    "    query_hash = query_to_hashed_vector(query)\n",
    "\n",
    "    # Convert the binary code list to a numpy array with dtype uint8\n",
    "    query_hash = np.array(query_hash, dtype=np.uint8)\n",
    "\n",
    "    # Ensure it's packed like the dataset hashes\n",
    "    query_hash_packed = np.packbits(query_hash)\n",
    "    query_hash_packed = np.expand_dims(query_hash_packed, axis=0)\n",
    "\n",
    "    # Perform the search\n",
    "    distances, indices = index.search(query_hash_packed, k)\n",
    "\n",
    "    # Retrieve the results\n",
    "    results = df.iloc[indices[0]].copy()\n",
    "    results['hamming_distance'] = distances[0]\n",
    "\n",
    "    # Compute similarity (based on hamming distance)\n",
    "    max_distance = num_bits\n",
    "    results['similarity'] = 1 - (results['hamming_distance'] / max_distance)\n",
    "\n",
    "    return results\n",
    "\n",
    "def hybrid_search_hash(query, index, df, k=10, alpha=0.7):\n",
    "    # Perform the semantic search\n",
    "    semantic_results = semantic_search_hash(query, index, df, k)\n",
    "    semantic_results['semantic_score'] = semantic_results['similarity']\n",
    "\n",
    "    # Reformat the semantic results to match keyword format\n",
    "    semantic_results = semantic_results.rename(columns={'Producturl': 'product_url', 'name': 'product_name', 'price': 'product_price', 'description': 'product_description', 'images': 'product_images'})\n",
    "    # Perform the keyword search and structure the output into a DataFrame\n",
    "\n",
    "    keyword_results = keyword_search(query, index_name)\n",
    "\n",
    "    keyword_hits = keyword_results['hits']['hits']\n",
    "    keyword_data = [\n",
    "        {\n",
    "            'product_id': hit['_source']['product_id'],\n",
    "            'product_price': hit['_source']['price'],\n",
    "            'product_url': hit['_source']['product_url'],\n",
    "            'product_name': hit['_source'].get('product_name', ''),\n",
    "            'product_description': hit['_source'].get('product_description', ''),\n",
    "            'product_images': hit['_source'].get('images', []),  # Include images\n",
    "            'keyword_score': hit['_score']\n",
    "        }\n",
    "        for hit in keyword_hits\n",
    "    ]\n",
    "\n",
    "    # Convert keyword search results into a DataFrame\n",
    "    keyword_df = pd.DataFrame(keyword_data)\n",
    "\n",
    "    # Merge the semantic and keyword results by product_id\n",
    "    combined = pd.merge(semantic_results, keyword_df, on='product_id', how='outer')\n",
    "\n",
    "    # Fill missing scores for keyword and semantic\n",
    "    combined['semantic_score'] = combined['semantic_score'].fillna(0)\n",
    "    combined['keyword_score'] = combined['keyword_score'].fillna(0)\n",
    "\n",
    "    # Calculate the combined score\n",
    "    combined['combined_score'] = alpha * combined['semantic_score'] + (1 - alpha) * combined['keyword_score']\n",
    "\n",
    "    # Sort by combined score and return top k results\n",
    "    combined = combined.sort_values(by='combined_score', ascending=False)\n",
    "\n",
    "    return combined.head(k)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "query = \"I want some gym clothes\"\n",
    "results = hybrid_search_hash(query, index, df, k=10, alpha=0.7)\n",
    "columns_to_keep = [\n",
    "    \"product_id\", \"product_url_y\", \"product_name_y\", \"product_description_y\",\n",
    "    \"product_images_y\", \"keyword_score\", \"combined_score\",\"product_price_y\"\n",
    "]\n",
    "\n",
    "# Filter the results DataFrame to only include the columns you want\n",
    "filtered_results = results[columns_to_keep]\n",
    "\n",
    "# Convert the filtered DataFrame to a dictionary\n",
    "filtered_results_dict = filtered_results.to_dict(orient='records')\n",
    "\n",
    "# Convert the dictionary to a JSON string\n",
    "filtered_results_json = json.dumps(filtered_results_dict, indent=4)\n",
    "\n",
    "# Output the filtered JSON string\n",
    "print(filtered_results_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
