{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90843d87-5b4f-4634-87a0-3c4477960468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "try:\n",
    "    from nltk.corpus import wordnet\n",
    "    wordnet.ensure_loaded()\n",
    "except LookupError:\n",
    "    print(\"WordNet not found, please run nltk.download('wordnet') in your environment.\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')  # Download wordnet resource\n",
    "\n",
    "# Load spaCy's English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "csv_file_path = '/content/Dataset.csv'  # Update this with the actual file path\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Updated function to parse the Description field\n",
    "def parse_description(desc_str):\n",
    "    try:\n",
    "        if isinstance(desc_str, str):\n",
    "            # Handle possible leading/trailing whitespace\n",
    "            desc_str = desc_str.strip()\n",
    "            # Check if there's a list in the description\n",
    "            list_start = desc_str.find('[')\n",
    "            if list_start != -1:\n",
    "                # Extract the text before the list\n",
    "                pre_list_text = desc_str[:list_start].strip()\n",
    "                # Extract the list part\n",
    "                list_str = desc_str[list_start:]\n",
    "                # Use ast.literal_eval to safely evaluate the list string\n",
    "                desc_list = ast.literal_eval(list_str)\n",
    "                # Extract text from each dictionary\n",
    "                texts = []\n",
    "                for item in desc_list:\n",
    "                    for key, value in item.items():\n",
    "                        texts.append(str(value))\n",
    "                # Combine all texts into a single string\n",
    "                combined_text = ' '.join([pre_list_text] + texts)\n",
    "                return combined_text.strip()\n",
    "            else:\n",
    "                # No list found, return the description as is\n",
    "                return desc_str.strip()\n",
    "        else:\n",
    "            # desc_str is not a string (could be NaN), return empty string or handle accordingly\n",
    "            return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing Description: {e}\")\n",
    "        print(f\"Raw Description Value: {desc_str}\")  # Debug print\n",
    "        return \"\"\n",
    "\n",
    "# Apply the parsing function to the Description column\n",
    "df['combined_description'] = df['Description'].apply(parse_description)\n",
    "\n",
    "# Preprocessing function remains the same\n",
    "def preprocess_text(text):\n",
    "    # 1. Lowercasing\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. Remove brackets but retain content\n",
    "    text = re.sub(r'[\\(\\)\\[\\]\\{\\}]', '', text)\n",
    "\n",
    "    # 3. Remove unnecessary punctuation except hyphens and slashes\n",
    "    text = re.sub(r'[^\\w\\s\\-\\/]', ' ', text)\n",
    "\n",
    "    # 4. Tokenization using spaCy\n",
    "    doc = nlp(text)\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        if token.text in stop_words:\n",
    "            continue\n",
    "        # Preserve numbers with units (e.g., \"12-inch\", \"500ml\")\n",
    "        if re.match(r'^\\d+[-/]\\w+$', token.text):\n",
    "            tokens.append(token.text)\n",
    "        elif token.is_alpha or token.like_num:\n",
    "            # Lemmatize the token\n",
    "            lemma = lemmatizer.lemmatize(token.text)\n",
    "            tokens.append(lemma)\n",
    "\n",
    "    # 5. Join tokens back into a string\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "\n",
    "    return preprocessed_text\n",
    "\n",
    "# Apply preprocessing to the combined_description\n",
    "df['preprocessed_description'] = df['combined_description'].apply(preprocess_text)\n",
    "\n",
    "# Preprocess the 'name' field\n",
    "df['preprocessed_name'] = df['name'].apply(preprocess_text)\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nProcessed Results:\")\n",
    "print(df[['product_id', 'preprocessed_name', 'preprocessed_description']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2e6201-e5b0-46da-b9c6-2bb1af6e54ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required library\n",
    "!pip install -U sentence-transformers\n",
    "\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load your preprocessed DataFrame 'df'\n",
    "# Assuming 'df' already has 'preprocessed_name' and 'preprocessed_description'\n",
    "\n",
    "# Combine the preprocessed name and description\n",
    "df['text_for_vectorization'] = df['preprocessed_name'] + ' ' + df['preprocessed_description']\n",
    "\n",
    "# Load the pre-trained model\n",
    "model_name = 'all-mpnet-base-v2'\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Function to encode texts in batches\n",
    "def encode_texts_in_batches(texts, model, batch_size=64):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        batch_embeddings = model.encode(batch_texts)\n",
    "        embeddings.extend(batch_embeddings)\n",
    "    return embeddings\n",
    "\n",
    "# Encode the texts\n",
    "texts = df['text_for_vectorization'].tolist()\n",
    "vector_embeddings = encode_texts_in_batches(texts, model, batch_size=64)\n",
    "\n",
    "# Add embeddings to the DataFrame\n",
    "df['vector_embeddings'] = vector_embeddings\n",
    "\n",
    "# Verify the embeddings\n",
    "print(f\"Number of embeddings: {len(vector_embeddings)}\")\n",
    "print(f\"Embedding dimension: {len(vector_embeddings[0])}\")\n",
    "\n",
    "# Save the embeddings (optional)\n",
    "np.savez_compressed('product_embeddings.npz', embeddings=np.vstack(vector_embeddings), product_ids=df['product_id'].values)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2dab7c2c-6c33-4617-95f6-359b3fddb276",
   "metadata": {},
   "source": [
    "After this the code is of Making Hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d615a7b-87f1-4451-810f-8783f369adae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import joblib  # Import joblib to save the PCA model\n",
    "\n",
    "# Load the vector embeddings (assuming they are already in df['vector_embeddings'])\n",
    "embeddings = np.vstack(df['vector_embeddings'].values)\n",
    "\n",
    "# 1. Apply PCA to reduce dimensionality (optional, helps in reducing the vector size)\n",
    "pca_dims = 64  # Choose appropriate dimensions for PCA\n",
    "pca = PCA(n_components=pca_dims)\n",
    "embeddings_pca = pca.fit_transform(embeddings)\n",
    "\n",
    "# 2. ITQ Implementation\n",
    "def iterative_quantization(embeddings, num_bits):\n",
    "    \"\"\"\n",
    "    Apply ITQ to reduce embeddings to binary codes.\n",
    "\n",
    "    Args:\n",
    "        embeddings (np.array): Input embeddings (PCA-reduced).\n",
    "        num_bits (int): Number of bits for binary encoding.\n",
    "\n",
    "    Returns:\n",
    "        binary_codes (np.array): Binary codes.\n",
    "        rotation_matrix (np.array): Learned rotation matrix.\n",
    "    \"\"\"\n",
    "    # Step 1: Zero-center the data\n",
    "    embeddings_mean = np.mean(embeddings, axis=0)\n",
    "    centered_embeddings = embeddings - embeddings_mean\n",
    "\n",
    "    # Step 2: Initialize random rotation matrix\n",
    "    _, pca_dims = centered_embeddings.shape\n",
    "    rotation_matrix = np.random.randn(pca_dims, pca_dims)\n",
    "    rotation_matrix, _ = np.linalg.qr(rotation_matrix)  # QR decomposition to get orthogonal matrix\n",
    "\n",
    "    # Step 3: Perform ITQ iterations (typically 50 iterations)\n",
    "    num_iterations = 50\n",
    "    for i in range(num_iterations):\n",
    "        # Step 3.1: Rotate the embeddings\n",
    "        rotated_embeddings = np.dot(centered_embeddings, rotation_matrix)\n",
    "\n",
    "        # Step 3.2: Binarize (quantize)\n",
    "        binary_codes = np.sign(rotated_embeddings)\n",
    "\n",
    "        # Step 3.3: Optimize the rotation matrix\n",
    "        C = np.dot(centered_embeddings.T, binary_codes)\n",
    "        U, _, Vt = np.linalg.svd(C)\n",
    "        rotation_matrix = np.dot(U, Vt)\n",
    "\n",
    "    # Step 4: Final binary codes\n",
    "    final_rotated_embeddings = np.dot(centered_embeddings, rotation_matrix)\n",
    "    binary_codes = (final_rotated_embeddings > 0).astype(int)\n",
    "\n",
    "    return binary_codes, rotation_matrix, embeddings_mean\n",
    "\n",
    "# Choose the number of bits for hashing (e.g., 64-bit binary codes)\n",
    "num_bits = 64\n",
    "\n",
    "# Apply ITQ to the embeddings\n",
    "binary_codes, rotation_matrix, embeddings_mean = iterative_quantization(embeddings_pca, num_bits)\n",
    "\n",
    "# Save PCA model\n",
    "joblib.dump(pca, 'pca_model.joblib')\n",
    "\n",
    "# Save rotation matrix and embeddings mean\n",
    "np.save('rotation_matrix.npy', rotation_matrix)\n",
    "np.save('embeddings_mean.npy', embeddings_mean)\n",
    "\n",
    "# Convert binary codes to string or integer representation (optional)\n",
    "binary_strings = [''.join(map(str, binary_code)) for binary_code in binary_codes]\n",
    "binary_integers = [int(b_str, 2) for b_str in binary_strings]\n",
    "\n",
    "# Add binary codes to the DataFrame\n",
    "df['binary_hash'] = binary_strings\n",
    "df['binary_int'] = binary_integers\n",
    "\n",
    "# Save binary codes to file (optional)\n",
    "df[['product_id', 'binary_hash']].to_csv('binary_hashes.csv', index=False)import pandas as pd\n",
    "\n",
    "# Load the first CSV (the one with embeddings)\n",
    "embeddings_csv_path = 'binary_hashes.csv'  # Change this to the path of your first CSV\n",
    "embeddings_df = pd.read_csv(embeddings_csv_path)\n",
    "\n",
    "# Load the second CSV (the one with product details)\n",
    "product_details_csv_path = 'Dataset.csv'  # Change this to the path of your second CSV\n",
    "product_details_df = pd.read_csv(product_details_csv_path)\n",
    "\n",
    "# Ensure both DataFrames have a common column to concatenate on (e.g., product_id)\n",
    "# If they do, use the following line to merge based on 'product_id'\n",
    "final_df = pd.merge(embeddings_df, product_details_df, on='product_id', how='inner')\n",
    "\n",
    "# Save the final DataFrame to a new CSV file\n",
    "final_dataset_path = 'final_dataset.csv'\n",
    "final_df.to_csv(final_dataset_path, index=False)\n",
    "\n",
    "print(f\"Final dataset saved to {final_dataset_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
